{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_preprocess.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaE-6ZuU0CpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install langdetect\n",
        "!pip install tweet-preprocessor\n",
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gSl6-DFDCMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emoji_pattern = re.compile(\"[\"\n",
        "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "         u\"\\U000024C2-\\U0001F251\"\n",
        "         \"]+\", flags=re.UNICODE)\n",
        "\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])\n",
        "\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])\n",
        "emoticons = emoticons_happy.union(emoticons_sad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI0xfLio0V6h",
        "colab_type": "code",
        "outputId": "f8e89b46-6403-48f4-cabe-d18a926c2868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "import pandas as pd\n",
        "import langdetect\n",
        "import preprocessor as pre\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "#load data\n",
        "data = pd.read_csv(\"/content/drive/My Drive/00.csv\")\n",
        "df = data.loc[:,['entities.hashtags.0.text','entities.hashtags.1.text','retweet_count','text']]\n",
        "\n",
        "#remove nan rows\n",
        "df = df[pd.notnull(df['text'])]\n",
        "\n",
        "def clean_tweets(tweet):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(tweet)\n",
        "    \n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "    \n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_tweet = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "            filtered_tweet.append(w)\n",
        "    return ' '.join(filtered_tweet)\n",
        "\n",
        "df = df[pd.notnull(df['text'])]\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "  list2 = []\n",
        "  text = \" \".join(w for w in nltk.wordpunct_tokenize(row['text']) \\\n",
        "         if w.lower() in words or not w.isalpha())\n",
        "  text = pre.clean(str(text))\n",
        "  clean_text = clean_tweets(text)\n",
        "  list1 = list()\n",
        "  p = re.compile(\"[A-Z]|[a-z]\")\n",
        "  for x in clean_text:\n",
        "    if(x == ' '):\n",
        "      st = ''.join(list1)\n",
        "      list2.append(st)\n",
        "      list1 = []\n",
        "    if(p.match(x)):\n",
        "      list1.append(x)\n",
        "  st = ''.join(list1)\n",
        "  list2.append(st)\n",
        "  list2 = list(filter(None,list2)) \n",
        "  st1 = ' '.join(list2)\n",
        "  df['text'][index] = st1\n",
        "  df['text'][index] = re.sub(r'http\\S+', '', str(df['text'][index]))\n",
        "  \n",
        "for index,row in df.iterrows():\n",
        "  if(df.loc[index]['text'] == ''):\n",
        "    df.drop([index], inplace = True)\n",
        "print(df['text'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "0                                        Maxedout FKMnrKK\n",
            "3                                                       I\n",
            "4       SO EXCITED FOR TONIGHT MY DAD AROUND WAITING F...\n",
            "6                                      Me pagan karate de\n",
            "7                             YES x I tell much important\n",
            "8                                        Mi define mi soy\n",
            "9                  Ni en para la sale arriba de la bikini\n",
            "10                          nu met met thee ens half back\n",
            "12                                  Can go launch already\n",
            "13              Top Best Gangster Past struggling make It\n",
            "14      rcooley Cheap Color Change Test Could Revoluti...\n",
            "15                                            My suddenly\n",
            "16                               A las te das son para ti\n",
            "17      Wind W Barometer Steady Temperature C Rain tod...\n",
            "18                                         DoXQuinty link\n",
            "19                                       I go ahead smile\n",
            "20                                                sarayli\n",
            "21                                     MedeaWatson tu ami\n",
            "22                                                     es\n",
            "23             news feed everyone ask see stupid mine say\n",
            "24                    ElanaMoore given idea birthday card\n",
            "26                                              GYM suave\n",
            "27                                               I OFFICE\n",
            "28                                                      I\n",
            "29                                                Twitter\n",
            "30                                      EduardoCanales Lo\n",
            "31                                                    I u\n",
            "32                                    speech van Gent nog\n",
            "33      Throwback favorite place beach missing smarsha...\n",
            "34                                     dont afraid follow\n",
            "                              ...                        \n",
            "2895                      Well said Concise always better\n",
            "2896                social media plan These guide MLEEhkC\n",
            "2898                                           per che ti\n",
            "2899                                                    k\n",
            "2900                                          You would I\n",
            "2902                                                thing\n",
            "2903                                               na dos\n",
            "2904                         ignorancee Lo io mi male che\n",
            "2905    KaranSoni I told already form said discuss tom...\n",
            "2907                                           evitje wat\n",
            "2908                                       na camera amor\n",
            "2909                sarahkuri Obviously What else would P\n",
            "2910            bitch use somebody else phone I got job u\n",
            "2911                                                check\n",
            "2912                                            eu mas eu\n",
            "2913                        starosta Si vas en ROSARIO el\n",
            "2914                                     also love YbKmEB\n",
            "2915                                                cVPXq\n",
            "2916                     I Millionaire Bay Group fcyXor D\n",
            "2917    Jordan Raging Bull Inspired Sticker Laced Loos...\n",
            "2918          TeamFollowProm IF YOU Want yyGhIW ADAY ADAY\n",
            "2919                                       mdswings thank\n",
            "2920                                               Fg via\n",
            "2921                          Me I going rest Me Wakes th\n",
            "2922                       strawbharryD Non l quei due di\n",
            "2923                       year seem fetish looking floor\n",
            "2924                                  seriously like true\n",
            "2925                  ASOTPOLAND feat Ana Breathe Life In\n",
            "2927                                       The th fiPZgVR\n",
            "2928                                  camizamp yo las mas\n",
            "Name: text, Length: 2253, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrJ3VBLxB3FK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"preprocessed1.csv\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gylA6mBPGuPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}